{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_VOCAB= '../uncased_L-12_H-768_A-12/vocab.txt'\n",
    "BERT_INIT_CHKPNT = '../uncased_L-12_H-768_A-12/bert_model.ckpt'\n",
    "BERT_CONFIG = '../uncased_L-12_H-768_A-12/bert_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
      "Requirement already satisfied: six in c:\\users\\uschas\\anaconda3\\lib\\site-packages (from bert-tensorflow) (1.12.0)\n",
      "Installing collected packages: bert-tensorflow\n",
      "Successfully installed bert-tensorflow-1.0.1\n"
     ]
    }
   ],
   "source": [
    "##install bert if not already done\n",
    "!pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USCHAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USCHAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import re\n",
    "import bert\n",
    "import datetime as dt\n",
    "import pickle\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from bert import modeling\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "NON_SPAM_TEST = DATA_PATH +'nonspam-test/'\n",
    "NON_SPAM_TRAIN = DATA_PATH +'nonspam-train/'\n",
    "SPAM_TEST = DATA_PATH +'spam-test/'\n",
    "SPAM_TRAIN = DATA_PATH +'spam-test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_spam_test_list = os.listdir(NON_SPAM_TEST)\n",
    "non_spam_train_list = os.listdir(NON_SPAM_TRAIN)\n",
    "spam_test_list = os.listdir(SPAM_TEST)\n",
    "spam_train_list = os.listdir(SPAM_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.DataFrame(columns=['id','text','label','split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in non_spam_test_list:\n",
    "    with open(NON_SPAM_TEST+filename,\"r\") as fd:\n",
    "        text = fd.read()\n",
    "        ID=filename.split(\".\")[0]\n",
    "        label='no'\n",
    "        split='test'\n",
    "        df.loc[df.shape[0]]=[ID]+[text]+[label]+[split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in non_spam_train_list:\n",
    "    with open(NON_SPAM_TRAIN+filename,\"r\") as fd:\n",
    "        text = fd.read()\n",
    "        ID=filename.split(\".\")[0]\n",
    "        label='no'\n",
    "        split='train'\n",
    "        df.loc[df.shape[0]]=[ID]+[text]+[label]+[split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in spam_train_list:\n",
    "    with open(SPAM_TRAIN+filename,\"r\") as fd:\n",
    "        text = fd.read()\n",
    "        ID=filename.split(\".\")[0]\n",
    "        label='yes'\n",
    "        split='train'\n",
    "        df.loc[df.shape[0]]=[ID]+[text]+[label]+[split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in spam_test_list:\n",
    "    with open(SPAM_TEST+filename,\"r\") as fd:\n",
    "        text = fd.read()\n",
    "        ID=filename.split(\".\")[0]\n",
    "        label='yes'\n",
    "        split='test'\n",
    "        df.loc[df.shape[0]]=[ID]+[text]+[label]+[split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'using',\n",
       " 'the',\n",
       " 'bert',\n",
       " 'token',\n",
       " '##izer']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenization.validate_case_matches_checkpoint(True,BERT_INIT_CHKPNT)\n",
    "bert_tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=BERT_VOCAB, do_lower_case=True)\n",
    "bert_tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 740/740 [00:11<00:00, 63.14it/s]\n"
     ]
    }
   ],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from tqdm import tqdm\n",
    "preprocessed_synop = []\n",
    "for sentance in tqdm(df['text'].values):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    stemmed_sentence = []\n",
    "    for e in sentance.split():\n",
    "        if e.lower() not in stopwords:\n",
    "            s=(sno.stem(lemmatizer.lemmatize(e.lower()))).encode('utf8')#lemitizing and stemming each word\n",
    "            stemmed_sentence.append(s)\n",
    "    sentance = b' '.join(stemmed_sentence)\n",
    "    preprocessed_synop.append(sentance)\n",
    "    \n",
    "df['CleanedSynopsis']=preprocessed_synop #adding a column of CleanedText which displays the data after pre-processing of the review \n",
    "df['CleanedSynopsis']=df['CleanedSynopsis'].str.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['CleanedSynopsis']\n",
    "Y = df['label']\n",
    "\n",
    "y=np.zeros((Y.shape[0],1))\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if(Y[i]=='no'):\n",
    "        y[i]=1\n",
    "    else:\n",
    "        y[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_vect = yTrain\n",
    "y_test_vect = yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to run this cell:,  0:00:07.203728\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=10,max_features=20000,smooth_idf=True,norm=\"l2\",\\\n",
    "                              tokenizer=lambda x:bert_tokenizer.tokenize(x),sublinear_tf=False,ngram_range=(1,4))\n",
    "x_train_ngram = vectorizer.fit_transform(xTrain)\n",
    "x_test_ngram = vectorizer.transform(xTest)\n",
    "print(\"Time Taken to run this cell:, \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USCHAS\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_ngram, y_train_vect)\n",
    "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.972972972972973"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test_ngram)\n",
    "accuracy_score(y_test_vect, y_pred, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out ='../output/'\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(clf, open(out+filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xTrain, open(out + 'xTrain.pk', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
